{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RitikaDharamkarJ/test_repository/blob/main/Tweets_Predictions_modify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqZQO-tMwmbR"
      },
      "source": [
        "#**Tweets Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81iSUOugyvc8"
      },
      "source": [
        "Stopword removal is a common preprocessing step in NLP tasks such as text classification. Stopwords are words that are very common and do not carry much meaning, such as \"the\", \"and\", \"a\", etc. Removing stopwords helps to reduce the size of the dataset and to focus the analysis on the more meaningful words. The choice of stopwords to remove can depend on the specific task and the dataset being used, and different NLP libraries provide different lists of stopwords that can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiQ_m1Ci3Bg-",
        "outputId": "72a31c93-65ab-499a-b0a9-770a7e34a594"
      },
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtCuH8enqPrY",
        "outputId": "b32dafcf-31ed-4b23-e640-901cb9572e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Installing collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G1za3ILqPrd",
        "outputId": "099787b5-d7c5-49a2-c022-e16574aebffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XwOgdXhs8yg0"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.layers import Dense, Input, Embedding, Conv1D, MaxPool1D, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,precision_score,recall_score\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7o31_d3qPrh"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,precision_score,recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load your tweet data\n",
        "tweets_Data = pd.read_csv(\"/content/Cyberbullying_tweets_cleaned.xlsx\")\n",
        "tweets_Data.head()"
      ],
      "metadata": {
        "id": "msSUjbRu14jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "tweets_Data = pd.read_csv(\"/content/Cyberbullying_tweets_cleaned.xlsx\")\n",
        "tweets_Data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "YPFAIjQ9LOG6",
        "outputId": "3f3aa216-7304-4a72-8452-3c1c7f1c4815"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9b03d819054f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets_Data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Cyberbullying_tweets_cleaned.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtweets_Data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x9e in position 14: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzvErqr6qPrj",
        "outputId": "0878eb39-313b-490b-f6a7-ae3032123491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(47692, 2)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_Data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATkINVuBqPrl",
        "outputId": "2f131bda-49a9-4f16-fab5-8ccbd04019f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['age', 'ethnicity', 'gender', 'not_cyberbullying',\n",
              "       'other_cyberbullying', 'religion'], dtype=object)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Let check the classes of the target column\n",
        "np.unique(tweets_Data[\"cyberbullying_type\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko21tTlBqPrm"
      },
      "source": [
        "#let drop all rows where tageet column = 'age', 'ethnicity', 'gender', 'religion'\n",
        "\n",
        "#Remove all rows where the value in the column equals 'come'\n",
        "tweets_Data = tweets_Data[~tweets_Data['cyberbullying_type'].isin(['age', 'ethnicity', 'gender', 'religion'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ_W8_qjqPrn"
      },
      "source": [
        "#Verify that the rows have been removed\n",
        "np.unique(tweets_Data[\"cyberbullying_type\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT-li0iJqPro"
      },
      "outputs": [],
      "source": [
        "tweets_Data = tweets_Data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mhAkwJE9IhY"
      },
      "outputs": [],
      "source": [
        "text = tweets_Data.drop(\"cyberbullying_type\", axis=1)\n",
        "labels = tweets_Data[\"cyberbullying_type\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate metrics**"
      ],
      "metadata": {
        "id": "3FQyJq3w_NXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Print_metric(modelname,y_test,y_pred):\n",
        "    acc = accuracy_score(y_test,y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    pre = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test,y_pred,average='macro')\n",
        "\n",
        "    # Print results\n",
        "    print(\"Evaluation metrics of the \", modelname)\n",
        "    print(\"Accuracy: \", acc)\n",
        "    print(\"F1 Score: \", f1)\n",
        "    print(\"Precision: \", pre)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "EYnB3gbm_KMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx5ZnDlYv_6A"
      },
      "source": [
        "##**MLP (Multilayer Perceptron)**\n",
        "\n",
        "Basic implementation of an MLP (Multilayer Perceptron) for predicting cyberbullying from tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut7zE-pkDU17"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data by converting it to lowercase and encoding the labels\n",
        "text = tweets_Data[\"tweet_text\"].str.lower()\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(tweets_Data[\"cyberbullying_type\"])\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X-ipIn8D4Tp"
      },
      "outputs": [],
      "source": [
        "# Convert the text data into numerical representations using a bag-of-words model\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZBkU_oFD6O_",
        "outputId": "eeaa2c56-9c49-4343-ddbe-000442c8586c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, random_state=0)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=0)\n",
        "mlp.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzAjn9O1qPru"
      },
      "source": [
        "The error message is telling you that the target data is multiclass but you are using the \"binary\" average option for precision. In a multiclass classification problem, there are more than two classes and so you need to use one of the other average options (None, 'micro', 'macro', 'weighted') instead of \"binary\".\n",
        "\n",
        "You can update the precision_score function call to use 'macro' or 'weighted' average, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZp-FadaqPrv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX1HTZmlv_Rg"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "y_pred = mlp.predict(X_test)\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(\"Accuracy: \", accuracy)\n",
        "#print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k7ju1HTqPrw",
        "outputId": "87d12a6b-f0dd-4b28-82d1-20eac8429e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics of the  MLP\n",
            "Accuracy:  0.8098288354510134\n",
            "F1 Score:  0.8065200533576378\n",
            "Precision:  0.8073912549113796\n",
            "Recall:  0.8061433877641324\n",
            "Confusion Matrix: \n",
            " [[1576    2    6   21   22    2]\n",
            " [   3 1597    4    6    9    3]\n",
            " [   5    8 1311  105  119    9]\n",
            " [  22   10  104  784  571   54]\n",
            " [  16   14  101  485  931    7]\n",
            " [   2    4   12   55   30 1513]]\n"
          ]
        }
      ],
      "source": [
        "#Print Eveluation metric\n",
        "Print_metric(\"MLP\",y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N4Tanl9wK6j"
      },
      "source": [
        "This code uses a bag-of-words model to convert the text data into numerical representations, which are then fed into the MLP classifier. The MLP classifier is trained on the training data and evaluated on the test data. The accuracy of the model is calculated and a confusion matrix is produced to provide a more detailed analysis of the performance of the model.\n",
        "\n",
        "Please note that this is just a basic implementation, and there are many different ways to improve and optimize the model, including tuning the hyperparameters, using different text representation methods, and combining it with other machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSCT6EjpwV5r"
      },
      "source": [
        "##**LSTM (Long Short-Term Memory)**\n",
        "\n",
        "* LSTM (Long Short-Term Memory) for prediction, particularly in the case of sequential data, such as time series or text data.\n",
        "* LSTMs are a type of Recurrent Neural Network (RNN) that are designed to handle sequential data by incorporating memory cells that allow the network to remember previous inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy4t1FheAFZ0"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE-t6MC_AIuU"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbmoEPLsAKw6"
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to the same length\n",
        "max_len = max([len(x) for x in X_train])\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ6Qpm-QANpi",
        "outputId": "7ab986d9-49f7-43d0-fd49-4e2e9bd2409c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1191/1191 [==============================] - 2134s 2s/step - loss: -133.5364 - accuracy: 0.1665 - val_loss: -242.1561 - val_accuracy: 0.1700\n",
            "Epoch 2/5\n",
            "1191/1191 [==============================] - 2120s 2s/step - loss: -357.0473 - accuracy: 0.1665 - val_loss: -461.4809 - val_accuracy: 0.1700\n",
            "Epoch 3/5\n",
            "1191/1191 [==============================] - 2124s 2s/step - loss: -580.4653 - accuracy: 0.1665 - val_loss: -681.1412 - val_accuracy: 0.1700\n",
            "Epoch 4/5\n",
            "1191/1191 [==============================] - 3463s 3s/step - loss: -846.0859 - accuracy: 0.2440 - val_loss: -983.4026 - val_accuracy: 0.2960\n",
            "Epoch 5/5\n",
            "1191/1191 [==============================] - 2129s 2s/step - loss: -1110.2584 - accuracy: 0.3001 - val_loss: -1224.9827 - val_accuracy: 0.3141\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x258323b0af0>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index)+1, 128, input_length=max_len))\n",
        "model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1YaXkvwzzd",
        "outputId": "897cdbe6-e96f-4d6c-8734-443e1e48e914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "298/298 [==============================] - 36s 119ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data\n",
        "y_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_prob, axis=-1)\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(\"Accuracy: \", accuracy)\n",
        "#print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAqMNJqdqPr2",
        "outputId": "71b51942-cd25-4be0-94d3-a5762e7adf6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics of the  LSTM\n",
            "Accuracy:  0.171794602541216\n",
            "F1 Score:  0.04886937300235983\n",
            "Precision:  0.028632433756869333\n",
            "Recall:  0.16666666666666666\n",
            "Confusion Matrix: \n",
            " [[1636    0    0    0    0    0]\n",
            " [1619    0    0    0    0    0]\n",
            " [1544    0    0    0    0    0]\n",
            " [1592    0    0    0    0    0]\n",
            " [1573    0    0    0    0    0]\n",
            " [1559    0    0    0    0    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ntiuos\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#Print Eveluation metric\n",
        "Print_metric(\"LSTM\",y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot-f0cUww8rk"
      },
      "source": [
        "This code uses the Keras library to build and train an LSTM model for the prediction task. The text data is tokenized and padded to the same length, and then fed into the LSTM network. The network consists of an embedding layer, an LSTM layer, and a dense layer with a sigmoid activation function. The model is trained on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcSM_r9BxQHE"
      },
      "source": [
        "##**SVM**\n",
        "\n",
        "Support Vector Machines (SVM) is a popular supervised machine learning algorithm used for binary classification problems, including cyber bulling prediction from tweets. In an SVM model, a hyperplane is used to separate the data into two classes, positive and negative, by maximizing the margin between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1ds8QvF9KQK"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the tweets\n",
        "text = tweets_Data[\"tweet_text\"].str.lower()\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(tweets_Data[\"cyberbullying_type\"])\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWHqJj0l-rcl"
      },
      "outputs": [],
      "source": [
        "# Convert the text into numerical features using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1oq9Gz1-uvf"
      },
      "outputs": [],
      "source": [
        "# Train the SVM model\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict the class labels for the test set\n",
        "y_pred = svm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyBp0UrxxVzA",
        "outputId": "5491c7d3-2adb-4080-eaab-33d796b6d3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics of the  SVM\n",
            "Accuracy:  0.8317757009345794\n",
            "F1 Score:  0.831553174834692\n",
            "Precision:  0.835013386969003\n",
            "Recall:  0.8316201027813134\n",
            "Confusion Matrix: \n",
            " [[1577    2    1   15   19    0]\n",
            " [   3 1549    0    4   15    1]\n",
            " [   2    9 1326  112  123    5]\n",
            " [  54   11   45  826  598   49]\n",
            " [  14    9   81  344 1140    6]\n",
            " [   1    5    5   49   20 1503]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the accuracy of the model\n",
        "Print_metric(\"SVM\",y_test, y_pred)\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(\"Accuracy of SVM:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwJ2xFPIxm37"
      },
      "source": [
        "Note: The code above is just an example and may not provide the best results for cyber bulling prediction from tweets. The performance of the SVM model can be improved by tuning the hyperparameters and feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Hl_IAtxyPk"
      },
      "source": [
        "##**Convolutional Neural Networks**\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are a type of deep neural networks that are commonly used for image classification and other computer vision tasks. They can also be applied to natural language processing tasks, including cyber bullying prediction from tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjUOC2Db-n5l"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjMENMHo9NfI"
      },
      "outputs": [],
      "source": [
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpYC1LJ19QcU"
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to the same length\n",
        "x_train = pad_sequences(sequences_train, maxlen=50)\n",
        "x_test = pad_sequences(sequences_test, maxlen=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_vos-Xu9SQt"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding the labels\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajs2qV5wqPsD",
        "outputId": "2994c2b8-e567-4e1f-b705-355c30fce9f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 5, 1, ..., 5, 1, 3])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaDqGuxG9Whj"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "inputs = Input(shape=(50,))\n",
        "embedding = Embedding(1000, 128)(inputs)\n",
        "conv = Conv1D(32, 3, activation=\"relu\")(embedding)\n",
        "pool = MaxPool1D(3)(conv)\n",
        "global_pool = GlobalMaxPool1D()(pool)\n",
        "dense = Dense(32, activation=\"relu\")(global_pool)\n",
        "outputs = Dense(1, activation=\"sigmoid\")(dense)\n",
        "model = Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpSt_20y9Zgn"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V2L7g2p9b0F",
        "outputId": "bcea6425-21d0-4f43-b255-862cbce4755c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "953/953 [==============================] - 10s 10ms/step - loss: -879527.9375 - accuracy: 0.1683 - val_loss: -4539271.5000 - val_accuracy: 0.1625\n",
            "Epoch 2/5\n",
            "953/953 [==============================] - 7s 8ms/step - loss: -23559370.0000 - accuracy: 0.1680 - val_loss: -58188236.0000 - val_accuracy: 0.1625\n",
            "Epoch 3/5\n",
            "953/953 [==============================] - 8s 8ms/step - loss: -128463008.0000 - accuracy: 0.1680 - val_loss: -232762192.0000 - val_accuracy: 0.1625\n",
            "Epoch 4/5\n",
            "953/953 [==============================] - 8s 8ms/step - loss: -383518688.0000 - accuracy: 0.1680 - val_loss: -595498944.0000 - val_accuracy: 0.1625\n",
            "Epoch 5/5\n",
            "953/953 [==============================] - 8s 8ms/step - loss: -853221312.0000 - accuracy: 0.1680 - val_loss: -1213476992.0000 - val_accuracy: 0.1625\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x25837a90760>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjQRPWX3x6-N",
        "outputId": "0820b456-62b8-48d1-b779-be0bd9c7e3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "298/298 [==============================] - 1s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(x_test).round().astype(int).flatten()\n",
        "\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(\"Accuracy of CNN:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJcZdWyeqPsG",
        "outputId": "4f53b4c0-d4d5-4a77-b449-a02fcada32c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics of the  CNN\n",
            "Accuracy:  0.16843431691693794\n",
            "F1 Score:  0.0480512866600761\n",
            "Precision:  0.02807238615282299\n",
            "Recall:  0.16666666666666666\n",
            "Confusion Matrix: \n",
            " [[   0 1557    0    0    0    0]\n",
            " [   0 1604    0    0    0    0]\n",
            " [   0 1544    0    0    0    0]\n",
            " [   0 1620    0    0    0    0]\n",
            " [   0 1626    0    0    0    0]\n",
            " [   0 1572    0    0    0    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ntiuos\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "Print_metric(\"CNN\",y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0MU_pM9yXF9"
      },
      "source": [
        "Note: The code above is just an example and may not provide the best results for cyber bullying prediction from tweets. The performance of the CNN model can be improved by tuning the hyperparameters and feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrc68r7zWRz"
      },
      "source": [
        "##**XGBoot and CatBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otOuq5pDzb5L"
      },
      "source": [
        "* CatBoost and XGBoost are both popular gradient boosting algorithms used for various machine learning tasks, including text classification. In the case of predicting cyberbullying from tweets, both CatBoost and XGBoost can be used to train a model on a labeled dataset of tweets and predict whether a new tweet contains cyberbullying or not.\n",
        "\n",
        "* To use CatBoost for this task, one can first preprocess the tweets to convert them into numerical features, such as word embeddings, that can be fed into the CatBoost model. Then, the model can be trained on the labeled dataset and evaluated on a held-out test set. One can tune the hyperparameters of the model using techniques such as cross-validation to achieve the best performance.\n",
        "\n",
        "* Similarly, XGBoost can also be used for this task. XGBoost is known for its ability to handle sparse datasets and handle categorical variables well, which can be useful in the context of text classification. As with CatBoost, one would preprocess the tweets and convert them into numerical features, then train an XGBoost model on the labeled dataset and evaluate it on a held-out test set. Hyperparameter tuning can be performed to achieve the best performance.\n",
        "\n",
        "* Both CatBoost and XGBoost can provide high accuracy and good performance in this task, and the choice between the two would depend on the specifics of the dataset and the problem being solved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs1JEOGlzjmo"
      },
      "source": [
        "To use CatBoost and XGBoost for predicting cyberbullying in tweets, you would need to perform the following steps:\n",
        "\n",
        "* Preprocessing: Clean and preprocess the tweet data, including removing stopwords, punctuation, and special characters. You may also want to convert the tweets into numerical features such as word embeddings or term frequency-inverse document frequency (TF-IDF) vectors.\n",
        "\n",
        "* Split data: Split the preprocessed data into a training set and a test set. The training set will be used to train the models, and the test set will be used to evaluate the models.\n",
        "\n",
        "* Training: Train CatBoost and XGBoost models on the training data. You can use grid search or other hyperparameter tuning techniques to find the best hyperparameters for each model.\n",
        "\n",
        "* Evaluation: Evaluate the performance of each model on the test data. You can use metrics such as accuracy, precision, recall, and F1 score to compare the models.\n",
        "\n",
        "* Prediction: Once you have a well-performing model, you can use it to predict whether new tweets contain cyberbullying or not.\n",
        "\n",
        "It's important to note that the specific implementation of each step can vary depending on the specifics of the dataset and the problem. However, these are the general steps for using CatBoost and XGBoost for text classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu6y37HH0ho6"
      },
      "source": [
        "**XGBoost and CatBoost to predict cyberbullying in tweets using Python:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6VkUNrd0wOe"
      },
      "source": [
        "* In this example, the tweets are first preprocessed by converting them into TF-IDF features.\n",
        "* Then, the data is split into a training set and a test set.\n",
        "* The XGBoost and CatBoost models are trained on the training data, and their performance is evaluated on the test data using accuracy and F1 score metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtP5IjWo1ck4"
      },
      "source": [
        "**Perform grid search with cross-validation to tune hyperparameters for both XGBoost and CatBoost:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onoqDmT917fL"
      },
      "outputs": [],
      "source": [
        "# define the features and target\n",
        "#X = tweets_Data .drop(\"cyberbullying_type\", axis=1)\n",
        "#y = tweets_Data [\"cyberbullying_type\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxlIYVW438km"
      },
      "outputs": [],
      "source": [
        "# Preprocessing: Convert tweets to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(tweets_Data[\"tweet_text\"])\n",
        "y = tweets_Data[\"cyberbullying_type\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "anjo179htd5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeNIkPNs1_Iv"
      },
      "outputs": [],
      "source": [
        "# specify the hyperparameters to be tuned and their search ranges\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [2, 3, 4],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'reg_lambda': [0.1, 1.0, 10.0],\n",
        "}\n",
        "cat_param_grid = {\n",
        "    'depth': [2, 3, 4],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bylevel': [0.8, 1.0],\n",
        "    'reg_lambda': [0.1, 1.0, 10.0],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao8fpPHL2B-Q"
      },
      "outputs": [],
      "source": [
        "# initialize the XGBoost classifier\n",
        "xgb_model = XGBClassifier()\n",
        "cat_model = CatBoostClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-pbsGB_2G1s"
      },
      "outputs": [],
      "source": [
        "# perform grid search with cross-validation for XGBoost98546+\n",
        "xgb_grid = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='accuracy',n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "xgb_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GBOTDl6utqIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UccrMCmhIbk-"
      },
      "outputs": [],
      "source": [
        "# display the best hyperparameters and performance\n",
        "print(\"Best hyperparameters and performance for XGBoost:\")\n",
        "print(xgb_grid.best_params_)\n",
        "print(xgb_grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYDcLiqfFRXG"
      },
      "outputs": [],
      "source": [
        "# perform grid search with cross-validation for CatBoost\n",
        "cat_grid = GridSearchCV(cat_model, cat_param_grid, cv=5, scoring='accuracy',n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2n1QUYSHvbFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTVTC5fb1fMJ"
      },
      "outputs": [],
      "source": [
        "# display the best hyperparameters and performance\n",
        "print(\"Best hyperparameters and performance for CatBoost:\")\n",
        "print(cat_grid.best_params_)\n",
        "print(cat_grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1vXJqO31kfl"
      },
      "source": [
        "In this example, GridSearchCV is used to perform a search over the specified hyperparameter ranges with cross-validation (cv=5) to evaluate the performance of the models. The best hyperparameters and performance for both XGBoost and CatBoost will be displayed after the grid search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtFl2WY446qg"
      },
      "source": [
        "**Train Best Hyperparamter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMo2t0y96RFj"
      },
      "outputs": [],
      "source": [
        "# Preprocessing: Convert tweets to numerical features using TF-IDF\n",
        "#vectorizer = TfidfVectorizer()\n",
        "#tweets = vectorizer.fit_transform(tweets_Data[\"tweet\"])\n",
        "#labels = tweets_Data[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QahS3yqB8Mh0"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5wd75iN8QL9"
      },
      "outputs": [],
      "source": [
        "# Train XGBoost model\n",
        "xgb = XGBClassifier(xgb_grid.best_params_)\n",
        "xgb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-OKpX9c8SkZ"
      },
      "outputs": [],
      "source": [
        "# Train CatBoost model\n",
        "cb = CatBoostClassifier(cat_grid.best_params_, verbose=0)\n",
        "cb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqvwyVng8U55"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test data\n",
        "xgb_preds = xgb.predict(X_test)\n",
        "cb_preds = cb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O-_mvX_qPsT"
      },
      "outputs": [],
      "source": [
        "# Print evaluation metrics\n",
        "Print_metric(\"XGBoost\",y_test, xgb_preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print evaluation metrics\n",
        "Print_metric(\"CatBoost\",y_test, cb_preds)"
      ],
      "metadata": {
        "id": "bR6rMBMD-7uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n0BL4-68W9B"
      },
      "source": [
        "#Evaluate models\n",
        "#XGBoost\n",
        "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
        "xgb_f1 = f1_score(y_test, xgb_preds)\n",
        "xgb_acc_pre = precision_score(y_test, xgb_preds)\n",
        "xgb_acc_recall = recall_score(y_test, xgb_preds)\n",
        "\n",
        "#Print results\n",
        "print(\"XGBoost Accuracy: \", xgb_acc)\n",
        "print(\"XGBoost F1 Score: \", xgb_f1)\n",
        "print(\"XGBoost Precision: \", xgb_acc_pre)\n",
        "print(\"XGBoost Recall: \", xgb_acc_recall)  \n",
        "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, xgb_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7INAkgapqPsU"
      },
      "source": [
        "#Evaluate models\n",
        "#Catboost\n",
        "cb_acc = accuracy_score(y_test, cb_preds)\n",
        "cb_f1 = f1_score(y_test, cb_preds)\n",
        "cb_pre = precision_score(y_test, cb_preds)\n",
        "cb_f1 = recall_score(y_test, cb_preds)\n",
        "\n",
        "#Print results\n",
        "print(\"CatBoost Accuracy: \", cb_acc)\n",
        "print(\"CatBoost F1 Score: \", cb_f1)\n",
        "print(\"CatBoost Precision: \", cb_pre)\n",
        "print(\"CatBoost Recall: \", cb_f1)\n",
        "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, cb_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3li1Yu0Hei9"
      },
      "source": [
        "##**Bi-directional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GOMC-F-Hg_D"
      },
      "outputs": [],
      "source": [
        "# Load the data into a pandas dataframe\n",
        "data = pd.read_csv(\"tweets.csv\")\n",
        "\n",
        "# Preprocessing the data\n",
        "max_features = 20000 # number of unique words to use\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "list_tokenized_tweets = tokenizer.texts_to_sequences(data['text'])\n",
        "\n",
        "# Padding the sequences to have the same length\n",
        "maxlen = 100 # maximum number of words in a tweet\n",
        "X = pad_sequences(list_tokenized_tweets, maxlen=maxlen)\n",
        "\n",
        "# Defining the model\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, 128)(inp)\n",
        "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, data['label'], batch_size=32, epochs=10, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQYRvCCrIEEj"
      },
      "outputs": [],
      "source": [
        "# Load the test data into a pandas dataframe\n",
        "test_data = pd.read_csv(\"test_tweets.csv\")\n",
        "\n",
        "# Preprocess the test data in the same way as the training data\n",
        "list_tokenized_test_tweets = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_test = pad_sequences(list_tokenized_test_tweets, maxlen=maxlen)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the predicted probabilities to binary labels\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "acc = accuracy_score(test_data['label'], y_pred)\n",
        "prec = precision_score(test_data['label'], y_pred)\n",
        "recall = recall_score(test_data['label'], y_pred)\n",
        "f1 = f1_score(test_data['label'], y_pred)\n",
        "\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOWyWyfeJgw4"
      },
      "source": [
        "##**3 Layers LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xknYswKgJpLe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "# Load the data into a pandas dataframe\n",
        "data = pd.read_csv(\"tweets.csv\")\n",
        "\n",
        "# Preprocessing the data\n",
        "max_features = 20000 # number of unique words to use\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "list_tokenized_tweets = tokenizer.texts_to_sequences(data['text'])\n",
        "\n",
        "# Padding the sequences to have the same length\n",
        "maxlen = 100 # maximum number of words in a tweet\n",
        "X = pad_sequences(list_tokenized_tweets, maxlen=maxlen)\n",
        "\n",
        "# Defining the model\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, 128)(inp)\n",
        "x = LSTM(60, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(x)\n",
        "x = LSTM(60, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(x)\n",
        "x = LSTM(60, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, data['label'], batch_size=32, epochs=10, validation_split=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1pROovpJf-j"
      },
      "source": [
        "The number of neurons in each layer of the LSTM network is a hyperparameter that needs to be tuned based on the specific requirements of your task. In the code example provided, the number of neurons in each LSTM layer is set to 60. This was likely chosen as a reasonable starting point for this specific problem, but there's no universal rule for choosing the number of neurons in LSTM layers.\n",
        "\n",
        "In general, having more neurons in the LSTM layers can increase the capacity of the network to model complex relationships in the data, but it can also increase the risk of overfitting. On the other hand, having fewer neurons can limit the network's ability to model complex relationships but can reduce the risk of overfitting.\n",
        "\n",
        "The optimal number of neurons for a specific task is often determined through experimentation and tuning, for example, by trying different values and evaluating the model performance using a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAf6IbKDKhSu"
      },
      "source": [
        " ##**Transfer learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVCvCHSCKkWu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.models import Model\n",
        "from keras.applications import Xception\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "# Load the data into a pandas dataframe\n",
        "data = pd.read_csv(\"tweets.csv\")\n",
        "\n",
        "# Preprocessing the data\n",
        "max_features = 20000 # number of unique words to use\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "list_tokenized_tweets = tokenizer.texts_to_sequences(data['text'])\n",
        "\n",
        "# Padding the sequences to have the same length\n",
        "maxlen = 100 # maximum number of words in a tweet\n",
        "X = pad_sequences(list_tokenized_tweets, maxlen=maxlen)\n",
        "\n",
        "# Load the pre-trained Xception model\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=(maxlen,))\n",
        "\n",
        "# Add a custom head for the binary classification task\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Freeze the base model and compile the transfer learning model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the transfer learning model\n",
        "model.fit(X, data['label'], batch_size=32, epochs=10, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vODl4DkMqPsZ"
      },
      "source": [
        "The error is because the f1_score function from scikit-learn's metrics module expects binary class labels but the target variable in this case is multiclass. You can specify the average type to be 'macro' or 'weighted' like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28dcF9RjqPsa"
      },
      "outputs": [],
      "source": [
        "f1 = f1_score(y_test, y_pred, average='macro')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z11IDquiqPsb"
      },
      "source": [
        "Or you can set average to None to get the f1-score for each class and then average them manually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Sek-ktqPsb"
      },
      "outputs": [],
      "source": [
        "f1 = f1_score(y_test, y_pred, average=None)\n",
        "f1_mean = f1.mean()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}